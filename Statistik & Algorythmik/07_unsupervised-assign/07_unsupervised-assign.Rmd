---
title: "Unsupervised Learning"
subtitle: "Algorithmik und Statistik 1"
author: "Sascha Metzger - 1910837830"
institute: "FH Kufstein"
date: "2019-01-01 (updated: `r Sys.Date()`)"
---
  
```{r setup}
pacman::p_load(tidyverse, ggfortify)
wdbc <- read_csv(file = "data/wisc.csv")
```
  
  
# Unsupervised Learning

Für dieses Blatt verwenden wir den kompletten Wisconsin Datensatz `wisc.csv`. Wir betrachten aber nicht die Vorhersage der Klasse (meist entfernen wir die Spalte "class"), sondern versuchen Features zu analysiseren.

## Aufgabe 1 EDA [5 Punkte]

- Wie viele Beobachtungen haben eine gutartige oder bösartige Diagnose ?
- Was ist der Mittelwert der einzelnen numerischen Spalten?


- Was ist die sd jeder der numerischen Spalten?


- Wie sind die Variablen miteinander verbunden (Corrplot)?


- Was können wir bis jetzt daraus schliessen? (Welche Variablen sind besonders hoch korreliert?)

## Aufgabe 2 PCA [15 Punkte]

- Erstelle ein PCA-Modell und gib das Summary aus. (Hinweis, ohne Skalierung/Normalisierung) Funktion `princomp` (Hinweis: wir verwenden sowohl princomp als auch pr.comp - Unterschied in der Hilfe)

```{r}
wdbc.pcov <- NULL
```



- Erstelle einen Plot mit der funktion `biplot`.


- Interpretiere den Plot, was fällt auf? (Einfluss der ersten beiden Hauptkomponenten?)


- Plotte die Varianz erklärt für die Hauptkomponenten. Was fällt dabei auf (seltsames Ergebnis wird erwartet).





### Durchführung von PCA mit Korrelationsmatrix:
Wenn die Korrelationsmatrix zur Berechnung der Eigenwerte und Eigenvektoren(Hauptkomponenten) verwendet wird, verwenden wir die Funktion prcomp().


- Erstelle eine PCA mit *center* und *scale* mit der Funktion prcomp und interpretiere den output.
```{r}
wdbc.pr <- NULL
```

- Erstelle einen Plot wie oben, der den Anteil der Varianz-Explained jeder Hauptkomponente darstellt.


- Erstelle einen Scatterplot der ersten beiden Hauptkomponenten und füge die Klasse als Variable hinzu. Sind die Klassen separiert? Wenn ja, was können wir daraus schliessen?










### Berechnen Sie den Anteil der erklärten Varianz

- Berechnen Sie den Anteil der erklärten Varianz wie vorher und beantworten Sie wie viele Komponenten sind notwendig um 95% der Varianz zu erklären. (Hinweis `cumsum`)






### Variation der PCA, Entfernen korrelierter Prädiktoren

Lassen Sie uns die gleiche Übung mit einem zweiten df durchführen, bei der wir die hoch korrelierten Prädiktoren (Corr>0.95) entfernt haben. (Center+Scale=T)

- Plotten Sie den Screeplot und interpretieren Sie die loadings.

```{r}
wdbc_cor <- NULL
```







- Plotten wir die ersten beiden Hauptkomponenten als Scatterplot und "färben" nach der Klasse. Sind die Klassen separiert?



- Um zu veranschaulichen, welche Variablen den größten Einfluss auf die ersten 2 Komponenten haben (Verwenden wir `autoplot` aus dem ggfortify package)



- Als Abschluss verwenden wir ggpairs um die ersten 3 Komponenten zu visualisieren.
- Was können wir daraus ablesen?

```{r, eval=F}
# Code intentionally Given
df_pcs <- cbind(as_tibble(wdbc_cor$class), as_tibble(cancer.pca2$x))
GGally::ggpairs(df_pcs, columns = 2:4, ggplot2::aes(color = value))
```





----------------------------

# Clustering

## Aufgabe 3 K-Means: Standardisiertes vs. nicht standardisiertes Clustering [5 Punkte]

```{r}
#wdbc_cluster <- wdbc[-1]

# Berechne die Ratio der Sum of Squares (Innerhalb/Zwischen Clustern) für k=1,2,3,4,5,6,7

# Erstelle einen Screeplot - Wo liegt der "Ellbogen"

# noch einmal für scalierte daten
wdbc_scaled <- NULL

# Interpretiere beide Varianten bei k=2 (warum wohl?)
```




## Aufgabe 4 Hierarchisches Clustering [5 Punkte]

- Erstelle die Distanz-Matrix für unskalierte und skalierte Daten.
- Verwende `hclust` um cluster zu erstellen.

```{r}
# distance matrix - (we use euclidean)

# create a "plain" hclust()-model for both matrices

# plot the resulting hierarchical clusters and interpret them
```



- Versuche "deinen" idealen Hierarchical Cluster zu bauen + begründe diesen. (Versuche verschiedene Distanz-Metriken, und verschiedene Inter-Cluster-Distanzen)

- Verwende `cutree()` um 4 Cluster zu bilden. 

