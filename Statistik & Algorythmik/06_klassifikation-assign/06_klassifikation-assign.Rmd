---
title: "Klassifikation"
subtitle: "Algorithmik und Statistik 1"
author: "Sascha Metzger - 1910837830"
institute: "FH Kufstein"
date: "2019-01-01 (updated: `r Sys.Date()`)"
---

Bitte um Beachtung der [Übungs-Policy](https://weblearn.fh-kufstein.ac.at/mod/page/view.php?id=64482) für genaue Anweisungen und einige Beurteilungsnotizen. Fehler bei der Einhaltung ergeben Punktabzug.

```{r setup}
library(statistics4ds)
library(tidyverse)

library(MASS)
library(caret)
library(knitr)
library(kableExtra)

# specific
library(e1071)
library(nnet)
library(ellipse)
```

```{r}
# helper function to caluculate the rmse for a knn
calc_err = function(actual, predicted) {
  mean(actual != predicted)
}

# import data
weatherAUS = read.csv("data/weatherAUS.csv")

wisc_trn = read.csv("data/wisc-trn.csv")
wisc_tst = read.csv("data/wisc-tst.csv")
```


# Aufgabe 1: Krebserkennung mit KNN [3 Punkte]

Für diese Übung werden wir Daten aus [`wisc-trn.csv`](wisc-trn.csv) und [`wisc-tst.csv`](wisc-tst.csv) verwenden, die jeweils Zug- und Testdaten enthalten. Dies ist eine Modifikation des Brustkrebs-Wisconsin-(Diagnose-)Datensatzes aus dem UCI Machine Learning Repository. Es wurden nur die ersten 10 Merkmalsvariablen bereitgestellt.

- [UCI](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
- [Data Detail](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names)

Sie sollten erwägen, die Antwort als Faktorvariable zu erzwingen. 

Betrachten Sie zwei verschiedene Vorverarbeitungs-Setups:

- **Setup 1**
    - Numerische Variablen nicht skaliert. 
- **Setup 2**
    - Numerische Variablen werden auf den Mittelwert 0 und die Standardabweichung 1 skaliert.

Für jeden Aufbau sind KNN-Modelle mit Werten von `k` von `1` bis `200` zu trainieren. Dabei werden nur die Variablen `Radius`, `Symmetrie` und `Textur` verwendet. Berechnen Sie für jede dieser Größen den Klassifizierungsfehler der Prüfung. Diese Ergebnisse sind in einer einzigen Darstellung zusammenzufassen, die den Prüffehler als Funktion von `k` darstellt. (Die Darstellung hat zwei "Kurven", eine für jeden Aufbau.) Ihre Darstellung sollte visuell ansprechend und gut beschriftet sein und eine Legende enthalten.

```{r}
# seed
set.seed(1910837830)

# data types erkennen   !!!!!!!!!!
# head(wisc_trn)

# k
k <- 1:200

# define model setups
wisc_knn_form_1 = as.formula(class ~ radius + symmetry + texture)
wisc_knn_form_2 = as.formula(class ~ scale(radius) + scale(symmetry) + scale(texture))

# get 200 different modules (from 1 to 200)
mod_1 = lapply(k, function(x) {knn3(wisc_knn_form_1, data = wisc_trn, k = x)})
mod_2 = lapply(k, function(x) {knn3(wisc_knn_form_2, data = wisc_trn, k = x)})
# get the predictions from each of our models
pred_1 = lapply(mod_1, predict, newdata = wisc_tst, type = "class")
pred_2 = lapply(mod_2, predict, newdata = wisc_tst, type = "class")
# calculate the errors for all 200 predictions
err_1 = sapply(pred_1, calc_err, actual = wisc_tst$class)
err_2 = sapply(pred_2, calc_err, actual = wisc_tst$class)

plot(k, err_1, type = "l", col = "green", xlab = "k", ylab = "error", ylim = c(0.08, 0.2))
lines(k, err_2, col = "orange")
legend("bottomright", c("mod_1", "mode_2"), lty = c(1, 1), col = c("green", "orange"))
```


# Aufgabe 2: Krebserkennung mit logistischer Regression [4 Punkte]

Wir verwenden den Datensatz und Split von Aufgabe 1.

Betrachten Sie eine additive logistische Regression, die *nur zwei Prädiktoren*, `Radius` und `Symmetrie`, berücksichtigt. Benutzen Sie dieses Modell zur Schätzung 

$$
p(x) = P(Y = \texttt{M} \mid X = x).
$$

Berichten Sie die Testsensitivität, Testspezifität und Testgenauigkeit für drei Klassifikatoren, wobei jeder einen anderen Grenzwert für die vorhergesagte Wahrscheinlichkeit verwendet:

$$
\hat{C}(x) =
\begin{cases} 
      M & \hat{p}(x) > c \\
      B & \hat{p}(x) \leq c
\end{cases}
$$

- $c = 0.1$
- $c = 0.5$
- $c = 0.9$

Wir werden `M` (maligne) als die "positive" Klasse betrachten, wenn wir die Sensitivität und Spezifität berechnen. Fassen Sie diese Ergebnisse in einer einzigen gut formatierten Tabelle zusammen.

```{r}
# kein hold out ?
# wir haben einen CUtoff -> wie wahrscheinlich muss er sein, dass er bösartig ist? 0,5 ist der Standard

# coerce to factor
wisc_trn$class = as.factor(wisc_trn$class)
wisc_tst$class = as.factor(wisc_tst$class)

# fit model
wisc_glm = glm(class ~ radius + symmetry, data = wisc_trn, family = "binomial")

# helper function, specific to glm function
get_pred_glm = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}

# get predicted values for each cutoff
pred_10 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.1, pos = "M", neg = "B")
pred_50 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.5, pos = "M", neg = "B")
pred_90 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.9, pos = "M", neg = "B")

# create table for each cutoff
tab_10 = table(predicted = pred_10, actual = wisc_tst$class)
tab_50 = table(predicted = pred_50, actual = wisc_tst$class)
tab_90 = table(predicted = pred_90, actual = wisc_tst$class)

# coerce tables to be confusion matrixes
con_mat_10 = confusionMatrix(tab_10, positive = "M")
con_mat_50 = confusionMatrix(tab_50, positive = "M")
con_mat_90 = confusionMatrix(tab_90, positive = "M")

metrics = rbind(
  
  c(con_mat_10$overall["Accuracy"], 
    con_mat_10$byClass["Sensitivity"], 
    con_mat_10$byClass["Specificity"]),
  c(con_mat_50$overall["Accuracy"], 
    con_mat_50$byClass["Sensitivity"], 
    con_mat_50$byClass["Specificity"]),
  c(con_mat_90$overall["Accuracy"], 
    con_mat_90$byClass["Sensitivity"], 
    con_mat_90$byClass["Specificity"])
)

rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
kable_styling(kable(metrics, format = "html", digits = 3), full_width = FALSE)

```


# Aufgabe 3: Wetter Logistische Regression [4 Punkte]
Kommen wir zur logistischen Regression. Sie werden wieder mit dem gleichen Wetterdatensatz arbeiten, aber das Ziel ist es, vorherzusagen, ob es morgen regnen wird. Wir haben Ihre Train- und Testsets für Sie erstellt. Ihre abhängigen Variablen sind die Features Humidity9am und Humidity3pm

Achtung die Daten sind nicht normalisiert, skalieren Sie diese gegebenfalls und geben Sie die Auswirkung der Normalisierung an.

## Anweisungen
- Erstellen und fitten Sie das logistic_model mit train
- Geben Sie die Genauigkeit Ihres Modells auf den Testdaten an.
- Betrachten Sie die Modell-Koeffizienten, was können Sie daraus schließen?

```{r}
set.seed(3456)

data_clean <- weatherAUS %>% na.omit()
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm) 

weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)

train <- df[ trainIndex, ] 
test  <- df[-trainIndex, ] 

logistic_model <- train(RainTomorrow ~ Humidity9am + Humidity3pm,  data=train, method="glm", family="binomial", metric = 'Accuracy')
```

```{r}
summary(logistic_model)

# Compute and print the accuracy
mod9am_c1_fit$results$Accuracy

# Assign and print the coefficents
summary(mod9am_c1_fit)

# Daten noch standardisieren!

# Since our features were normalized beforehand, we can look at the magnitude of our coefficients to tell us the importance of each independent variable. Here you can see the the second variable, Humidity3pm was much more important to our outcome than humidity from that morning. This is intuitive since we are trying to predict the rain for tomorrow!
```




# Aufgabe 4: Wetter Klassifikation Evaluation [4 Punkte]
In Fortführung der Evaluierungsmetriken werden Sie diesmal unser logistisches Regressionsmodell von früher evaluieren, mit dem Ziel, die binäre RainTomorrow-Feature mit Hilfe von 'Humidity'+"weiteren" vorherzusagen.

Wir haben das Modell als `logistic_model` vorher definiert. Generieren und analysieren Sie die Confusionmatrix und berechnen Sie dann Präzision und Recall, bevor Sie eine Schlussfolgerung ziehen.

## Anweisungen
- Generieren und drucken Sie die Confusionmatrix für Ihr Modell; identifizieren Sie die Fehler Typ I und Typ II für die Testdaten.
- Berechnen und geben Sie die Genauigkeit Ihres Models aus; können Sie erklären, warum Präzision in diesem Zusammenhang hilfreich ist?

```{r}
# Generate and output the confusion matrix
mod9am_pred <- predict(mod9am_c1_fit, test)
m <- confusionMatrix(mod9am_pred, test[,"RainTomorrow"])

# Compute and print the precision
m

# Compute and print the recall Recall = TruePositives / (TruePositives + FalseNegatives)
recall(m[2]$table)

```





# Aufgabe 5 Wetter Klassifikatoren vergleichen [5 Punkte]

Verwenden Sie die Daten in `weatherAUS`, nun mit allen Variablen, die Train- bzw. Testdaten enthalten (von vorher). Als Antwort ist `RainTomorrow` zu verwenden. Nach dem Import der Daten `RainTomorrow` als Faktor erzwingen, falls dies nicht bereits der Fall ist.

Erstellen Sie ein Pair-Plot für die Trainingsdaten und trainieren Sie dann die folgenden Modelle unter Verwendung der beiden verfügbaren Prädiktoren:

- 5 verschiedene Additive logistische Regression (Multinomiale Regression)
  - **Intercept**: $\log \left( \frac{p(x)}{1 - p(x)} \right) = \beta_0$
  - **Simple**: $\log \left( \frac{p(x)}{1 - p(x)} \right) = \beta_0 + \beta_1 \texttt{x}$
  - **Multiple**:$\log \left( \frac{p(x)}{1 - p(x)} \right) = \beta_0 + \beta_1 x_1  + \beta_2 x_2  + \beta_3 x_3$
  - **Additive**: Ein *additives* Modell mit (fast) allen ~5-10 verfügbaren Prädiktoren (Achtung: viele NAs, Faktoren z.B. Location)
  - **Interaction**: Ein *Interaktions* Modell, das alle Begriffe erster Ordnung (von Additive) und alle möglichen Wechselwirkungen in beide Richtungen enthält
  - LDA (mit aus Daten geschätzten Prioren)
  - LDA mit Flachprior
  - Naive Bayes (mit geschätzten Prioren aus den Daten)

Berechnen Sie Test- und Trainfehlerraten für jedes Modell. Fassen Sie diese Ergebnisse in einer einzigen gut formatierten Tabelle zusammen.

Interpretieren/visualisieren Sie die Modelle

```{r}
# pair plot
mod_intercept <- NULL
mod_simple <- NULL
mod_multiple <- NULL
mod_additive <- NULL # careful on this one!
mod_interaction <- NULL
```

# Aufgabe 7 Konzeptprüfung [6 Punkte]

Beantworten Sie die folgenden Fragen auf der Grundlage Ihrer Ergebnisse aus den Aufgaben **[je 1 Punkt]**.

**(a)** In Aufgabe 5, warum schneidet Naive Bayes schlecht ab?

**(b)** Basierend auf Ihren Ergebnissen in Aufgabe 1+2, welches dieser Modelle schneidet am besten ab?

**(c)** Basierend auf Ihren Ergebnissen in Aufgabe 1+2, welches dieser Modelle könnte Ihrer Meinung nach zu wenig geeignet sein?

**(d)** Welches dieser Modelle könnte Ihrer Meinung nach aufgrund Ihrer Ergebnisse in Aufgabe 5 überdimensioniert sein?

**(e)** Welche der Klassifikatoren aus Aufgaben 1+2 bevorzugen Sie?

**(f)** Nennen Sie die Metrik, die Sie für Ihre Entscheidung verwendet haben, teilweise **(e)**, und einen Grund für die Verwendung dieser Metrik.
  
  
