# get predictions from classifiers built using fitted models
auto_trn_pred = lapply(auto_trn_prob, function(x) {ifelse(RainTomorrow == "No", 0, 1)})
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
head(data)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
y <- train$RainTomorrow
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
auto_mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
auto_trn_prob = lapply(auto_mod_list, predict, newdata = train)
auto_tst_prob = lapply(auto_mod_list, predict, newdata = test)
# get predictions from classifiers built using fitted models
auto_trn_pred = lapply(auto_trn_prob, function(x) {ifelse(x == "No", 0, 1)})
auto_tst_pred = lapply(auto_tst_prob, function(x) {ifelse(x == "No", 0, 1)})
# calculate errors
auto_trn_err  = sapply(auto_trn_pred, calc_err, actual = train$RainTomorrow)
auto_tst_err  = sapply(auto_tst_pred, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = auto_trn_err,
tst_err = auto_tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
head(data_clean)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
y <- train$RainTomorrow
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
auto_mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
auto_trn_prob = lapply(auto_mod_list, predict, newdata = train)
auto_tst_prob = lapply(auto_mod_list, predict, newdata = test)
# get predictions from classifiers built using fitted models
auto_trn_pred = lapply(auto_trn_prob, function(x) {ifelse(x == "No", 0, 1)})
auto_tst_pred = lapply(auto_tst_prob, function(x) {ifelse(x == "No", 0, 1)})
# calculate errors
auto_trn_err  = sapply(auto_trn_pred, calc_err, actual = train$RainTomorrow)
auto_tst_err  = sapply(auto_tst_pred, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = auto_trn_err,
tst_err = auto_tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
head(data_clean)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
y <- train$RainTomorrow
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
auto_mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
auto_trn_prob = lapply(auto_mod_list, predict, newdata = train)
auto_tst_prob = lapply(auto_mod_list, predict, newdata = test)
# get predictions from classifiers built using fitted models
auto_trn_pred = lapply(auto_trn_prob, function(x) {ifelse(x = "No", 0, 1)})
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
head(data_clean)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
y <- train$RainTomorrow
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
auto_mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
auto_trn_prob = lapply(auto_mod_list, predict, newdata = train)
auto_tst_prob = lapply(auto_mod_list, predict, newdata = test)
# get predictions from classifiers built using fitted models
auto_trn_pred = lapply(auto_trn_prob, function(x) {ifelse(x == "No", 1, 0)})
auto_tst_pred = lapply(auto_tst_prob, function(x) {ifelse(x == "No", 1, 0)})
# calculate errors
auto_trn_err  = sapply(auto_trn_pred, calc_err, actual = train$RainTomorrow)
auto_tst_err  = sapply(auto_tst_pred, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = auto_trn_err,
tst_err = auto_tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
head(data_clean)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
y <- train$RainTomorrow
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
auto_mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
auto_trn_prob = lapply(auto_mod_list, predict, newdata = train)
auto_tst_prob = lapply(auto_mod_list, predict, newdata = test)
# get predictions
auto_trn_pred = lapply(auto_trn_prob, function(x) {ifelse(x == "No", 1, 0)})
auto_tst_pred = lapply(auto_tst_prob, function(x) {ifelse(x == "No", 1, 0)})
# calculate errors
auto_trn_err  = sapply(auto_trn_prob, calc_err, actual = train$RainTomorrow)
auto_tst_err  = sapply(auto_tst_prob, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = auto_trn_err,
tst_err = auto_tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
head(data_clean)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
y <- train$RainTomorrow
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
trn_prob = lapply(mod_list, predict, newdata = train)
tst_prob = lapply(mod_list, predict, newdata = test)
# get predictions and calculate errors
trn_err  = sapply(trn_prob, calc_err, actual = train$RainTomorrow)
tst_err  = sapply(tst_prob, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = trn_err,
tst_err = tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
trn_prob = lapply(mod_list, predict, newdata = train)
tst_prob = lapply(mod_list, predict, newdata = test)
# get predictions and calculate errors
trn_err  = sapply(trn_prob, calc_err, actual = train$RainTomorrow)
tst_err  = sapply(tst_prob, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = trn_err,
tst_err = tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
install.packages("ISLR")
install.packages(remotes)
remotes::install_github("g4challenge/statistics-for-data-scientists")
install.packages("remotes")
install.packages(remotes)
remotes::install_github("g4challenge/statistics-for-data-scientists")
library(statistics4ds)
library(tidyverse)
library(MASS)
library(caret)
library(knitr)
library(kableExtra)
# specific
library(e1071)
library(nnet)
library(ellipse)
library(statistics4ds)
library(tidyverse)
library(MASS)
library(caret)
library(knitr)
library(kableExtra)
# specific
library(e1071)
library(nnet)
library(ellipse)
# helper function to caluculate the rmse for a knn
calc_err = function(actual, predicted) {
mean(actual != predicted)
}
# import data
weatherAUS = read.csv("data/weatherAUS.csv")
wisc_trn = read.csv("data/wisc-trn.csv")
wisc_tst = read.csv("data/wisc-tst.csv")
# seed
set.seed(1910837830)
# data types erkennen   !!!!!!!!!!
# head(wisc_trn)
# k
k <- 1:200
# define model setups
wisc_knn_form_1 = as.formula(class ~ radius + symmetry + texture)
wisc_knn_form_2 = as.formula(class ~ scale(radius) + scale(symmetry) + scale(texture))
# get 200 different modules (from 1 to 200)
mod_1 = lapply(k, function(x) {knn3(wisc_knn_form_1, data = wisc_trn, k = x)})
mod_2 = lapply(k, function(x) {knn3(wisc_knn_form_2, data = wisc_trn, k = x)})
# get the predictions from each of our models
pred_1 = lapply(mod_1, predict, newdata = wisc_tst, type = "class")
pred_2 = lapply(mod_2, predict, newdata = wisc_tst, type = "class")
# calculate the errors for all 200 predictions
err_1 = sapply(pred_1, calc_err, actual = wisc_tst$class)
err_2 = sapply(pred_2, calc_err, actual = wisc_tst$class)
plot(k, err_1, type = "l", col = "green", xlab = "k", ylab = "error", ylim = c(0.08, 0.2))
lines(k, err_2, col = "orange")
legend("bottomright", c("mod_1", "mode_2"), lty = c(1, 1), col = c("green", "orange"))
wisc_trn$class = as.factor(wisc_trn$class)
wisc_tst$class = as.factor(wisc_tst$class)
# fit model
wisc_glm = glm(class ~ radius + symmetry, data = wisc_trn, family = "binomial")
# helper function, specific to glm function
get_pred_glm = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
probs = predict(mod, newdata = data, type = "response")
ifelse(probs > cut, pos, neg)
}
# get predicted values for each cutoff
pred_10 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.1, pos = "M", neg = "B")
pred_50 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.5, pos = "M", neg = "B")
pred_90 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.9, pos = "M", neg = "B")
# create table for each cutoff
tab_10 = table(predicted = pred_10, actual = wisc_tst$class)
tab_50 = table(predicted = pred_50, actual = wisc_tst$class)
tab_90 = table(predicted = pred_90, actual = wisc_tst$class)
# coerce tables to be confusion matrixes
con_mat_10 = confusionMatrix(tab_10, positive = "M")
con_mat_50 = confusionMatrix(tab_50, positive = "M")
con_mat_90 = confusionMatrix(tab_90, positive = "M")
metrics = rbind(
c(con_mat_10$overall["Accuracy"],
con_mat_10$byClass["Sensitivity"],
con_mat_10$byClass["Specificity"]),
c(con_mat_50$overall["Accuracy"],
con_mat_50$byClass["Sensitivity"],
con_mat_50$byClass["Specificity"]),
c(con_mat_90$overall["Accuracy"],
con_mat_90$byClass["Sensitivity"],
con_mat_90$byClass["Specificity"])
)
rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
kable_styling(kable(metrics, format = "html", digits = 3), full_width = FALSE)
set.seed(1910837830)
data_clean <- weatherAUS %>% na.omit()
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- df[ trainIndex, ]
test  <- df[-trainIndex, ]
logistic_model <- train(RainTomorrow ~ Humidity9am + Humidity3pm,  data=train, method="glm", family="binomial", metric = 'Accuracy')
summary(logistic_model)
# Compute and print the accuracy
logistic_model$results$Accuracy
# Assign and print the coefficents
summary(logistic_model)
# Daten noch standardisieren!
# Since our features were normalized beforehand, we can look at the magnitude of our coefficients to tell us the importance of each independent variable. Here you can see the the second variable, Humidity3pm was much more important to our outcome than humidity from that morning. This is intuitive since we are trying to predict the rain for tomorrow!
# Generate and output the confusion matrix
mod9am_pred <- predict(logistic_model, test)
m <- confusionMatrix(mod9am_pred, test[,"RainTomorrow"])
# Compute and print the precision
m
# Compute and print the recall Recall = TruePositives / (TruePositives + FalseNegatives)
recall(m[2]$table)
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
# fit models
mod_intercept =   glm(RainTomorrow ~ 1, data = data_clean, family = "binomial")
mod_simple =      glm(RainTomorrow ~ Humidity3pm, data = data_clean, family = "binomial")
mod_multiple =    glm(RainTomorrow ~ Humidity3pm + Humidity9am, data = data_clean, family = "binomial")
mod_additive =    glm(RainTomorrow ~ (Humidity3pm + Humidity9am), data = data_clean, family = "binomial")
mod_interaction = glm(RainTomorrow ~ (Humidity3pm + Humidity9am) ^ 2, data = data_clean, family = "binomial")
# create model list
mod_list = list(mod_intercept, mod_simple, mod_multiple, mod_additive, mod_interaction)
# get predicted probabilities from fitted models
trn_prob = lapply(mod_list, predict, newdata = train)
tst_prob = lapply(mod_list, predict, newdata = test)
# get predictions and calculate errors
trn_err  = sapply(trn_prob, calc_err, actual = train$RainTomorrow)
tst_err  = sapply(tst_prob, calc_err, actual = test$RainTomorrow)
# create df of results
results = data.frame(
name = c("**Intercept**", "**Simple**", "**Multiple**", "**Additive**", "**Interaction**"),
trn_err = trn_err,
tst_err = tst_err
)
colnames(results) = c("Model Name", "Train Error", "Test Error")
# create results table
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
wisc_trn$class = as.factor(wisc_trn$class)
wisc_tst$class = as.factor(wisc_tst$class)
# fit model
wisc_glm = glm(class ~ radius + symmetry, data = wisc_trn, family = "binomial")
# helper function, specific to glm function
get_pred_glm = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
probs = predict(mod, newdata = data, type = "response")
ifelse(probs > cut, pos, neg)
}
# get predicted values for each cutoff
pred_10 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.1, pos = "M", neg = "B")
pred_50 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.5, pos = "M", neg = "B")
pred_90 = get_pred_glm(wisc_glm, wisc_tst, res = "class", cut = 0.9, pos = "M", neg = "B")
# create table for each cutoff
tab_10 = table(predicted = pred_10, actual = wisc_tst$class)
tab_50 = table(predicted = pred_50, actual = wisc_tst$class)
tab_90 = table(predicted = pred_90, actual = wisc_tst$class)
# coerce tables to be confusion matrixes
con_mat_10 = confusionMatrix(tab_10, positive = "M")
con_mat_50 = confusionMatrix(tab_50, positive = "M")
con_mat_90 = confusionMatrix(tab_90, positive = "M")
metrics = rbind(
c(con_mat_10$overall["Accuracy"],
con_mat_10$byClass["Sensitivity"],
con_mat_10$byClass["Specificity"]),
c(con_mat_50$overall["Accuracy"],
con_mat_50$byClass["Sensitivity"],
con_mat_50$byClass["Specificity"]),
c(con_mat_90$overall["Accuracy"],
con_mat_90$byClass["Sensitivity"],
con_mat_90$byClass["Specificity"])
)
metrics
rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
kable_styling(kable(metrics, format = "html", digits = 3), full_width = FALSE)
# seed
set.seed(1910837830)
# data types erkennen   !!!!!!!!!!
# head(wisc_trn)
# k
k <- 1:200
# define model setups
wisc_knn_form_1 = as.formula(class ~ radius + symmetry + texture)
wisc_knn_form_2 = as.formula(class ~ scale(radius) + scale(symmetry) + scale(texture))
# get 200 different modules (from 1 to 200)
mod_1 = lapply(k, function(x) {knn3(wisc_knn_form_1, data = wisc_trn, k = x)})
mod_2 = lapply(k, function(x) {knn3(wisc_knn_form_2, data = wisc_trn, k = x)})
# get the predictions from each of our models
pred_1 = lapply(mod_1, predict, newdata = wisc_tst, type = "class")
pred_2 = lapply(mod_2, predict, newdata = wisc_tst, type = "class")
# calculate the errors for all 200 predictions
err_1 = sapply(pred_1, calc_err, actual = wisc_tst$class)
err_2 = sapply(pred_2, calc_err, actual = wisc_tst$class)
err_1
plot(k, err_1, type = "l", col = "green", xlab = "k", ylab = "error", ylim = c(0.08, 0.2))
lines(k, err_2, col = "orange")
legend("bottomright", c("mod_1", "mode_2"), lty = c(1, 1), col = c("green", "orange"))
# seed
set.seed(1910837830)
wisc_trn$class = as.factor(wisc_trn$class)
wisc_tst$class = as.factor(wisc_tst$class)
# k
k <- 1:200
# define model setups
wisc_knn_form_1 = as.formula(class ~ radius + symmetry + texture)
wisc_knn_form_2 = as.formula(class ~ scale(radius) + scale(symmetry) + scale(texture))
# get 200 different modules (from 1 to 200)
mod_1 = lapply(k, function(x) {knn3(wisc_knn_form_1, data = wisc_trn, k = x)})
mod_2 = lapply(k, function(x) {knn3(wisc_knn_form_2, data = wisc_trn, k = x)})
# get the predictions from each of our models
pred_1 = lapply(mod_1, predict, newdata = wisc_tst, type = "class")
pred_2 = lapply(mod_2, predict, newdata = wisc_tst, type = "class")
# calculate the errors for all 200 predictions
err_1 = sapply(pred_1, calc_err, actual = wisc_tst$class)
err_2 = sapply(pred_2, calc_err, actual = wisc_tst$class)
plot(k, err_1, type = "l", col = "green", xlab = "k", ylab = "error", ylim = c(0.08, 0.2))
lines(k, err_2, col = "orange")
legend("bottomright", c("mod_1", "mode_2"), lty = c(1, 1), col = c("green", "orange"))
library(statistics4ds)
library(tidyverse)
library(MASS)
library(caret)
library(knitr)
library(kableExtra)
# specific
library(e1071)
library(nnet)
library(ellipse)
# define flat prior
flat = c(1, 1) / 2
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
# fit models
mod_intercept <- glm(RainTomorrow ~ 1, data = train, family = "binomial")
mod_simple <- glm(RainTomorrow ~ RainToday, data = train, family = "binomial")
mod_multiple <- glm(RainTomorrow ~  Humidity9am + Humidity3pm + Pressure9am + Pressure3pm , data=train_weather, family="binomial")
# define flat prior
flat = c(1, 1) / 2
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
# fit models
mod_intercept <- glm(RainTomorrow ~ 1, data = train, family = "binomial")
mod_simple <- glm(RainTomorrow ~ RainToday, data = train, family = "binomial")
mod_multiple <- glm(RainTomorrow ~  Humidity9am + Humidity3pm + Pressure9am + Pressure3pm , data=train, family="binomial")
mod_additive <- gam(RainTomorrow ~ Evaporation + MinTemp + Sunshine + Humidity3pm + Pressure3pm + RainToday, data = train, family = "binomial", na.omit=TRUE)
# define flat prior
flat = c(1, 1) / 2
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
# fit models
mod_intercept <- glm(RainTomorrow ~ 1, data = train, family = "binomial")
mod_simple <- glm(RainTomorrow ~ RainToday, data = train, family = "binomial")
mod_multiple <- glm(RainTomorrow ~  Humidity9am + Humidity3pm + Pressure9am + Pressure3pm , data=train, family="binomial")
mod_additive <- glm(RainTomorrow ~ Evaporation + MinTemp + Sunshine + Humidity3pm + Pressure3pm + RainToday, data = train, family = "binomial", na.omit=TRUE)
# define flat prior
flat = c(1, 1) / 2
data_clean <- weatherAUS %>% na.omit()
data_clean$RainTomorrow = as.factor(data_clean$RainTomorrow)
df <- data.frame(RainTomorrow = data_clean$RainTomorrow, Humidity9am = data_clean$Humidity9am, Humidity3pm = data_clean$Humidity3pm)
weatherAUS_n <- data_clean[sample(nrow(data_clean), 1000),]
trainIndex <- createDataPartition(df$RainTomorrow, p = .75,  list = FALSE,  times = 1)
train <- data_clean[ trainIndex, ]
test  <- data_clean[-trainIndex, ]
# fit models
mod_intercept <- glm(RainTomorrow ~ 1, data = train, family = "binomial")
mod_simple <- glm(RainTomorrow ~ RainToday, data = train, family = "binomial")
mod_multiple <- glm(RainTomorrow ~  Humidity9am + Humidity3pm + Pressure9am + Pressure3pm , data=train, family="binomial")
mod_additive <- glm(RainTomorrow ~ Evaporation + MinTemp + Sunshine + Humidity3pm + Pressure3pm + RainToday, data = train, family = "binomial")
mod_interaction <- glm(RainTomorrow ~ (Evaporation + MinTemp + Sunshine + Sunshine + Humidity3pm + Pressure3pm + RainToday)^2 , data = train, family = "binomial")
mod_lda <- lda(RainTomorrow ~ .,data=train)
