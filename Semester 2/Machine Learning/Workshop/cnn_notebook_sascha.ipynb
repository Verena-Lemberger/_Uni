{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit17a369aea379459798f6ccae7683a0c4",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow stuff\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPool2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the mnist dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "# load the dataset and split it into train and test data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# imgs size (28 x 28 Pixel)\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# was ist x_train.shape[0] ???\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# convert training and test data to floats \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# 255 because of greyscaled images (black & white)\n",
    "x_train = x_train / 255  \n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_21 (Conv2D)           (None, 26, 26, 1)         10        \n_________________________________________________________________\nflatten_10 (Flatten)         (None, 676)               0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 10)                6770      \n=================================================================\nTotal params: 6,780\nTrainable params: 6,780\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# creates a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# add a layer to the model\n",
    "model.add(\n",
    "    # create a 2D convolution layer\n",
    "    Conv2D(\n",
    "        input_shape = input_shape, # only needed in the input layer\n",
    "        \n",
    "        # required parameters\n",
    "        filters = 1, # dimensionality of the output space -> output params = input classes * filters\n",
    "        kernel_size = (3, 3), \n",
    "        \n",
    "        # optional parameters\n",
    "        activation = relu, # applies the rectified linear unit activation function, default is the linear activation function\n",
    "\n",
    "        # strides = (1, 1), # for specifying the strides of the convolution along the height and width\n",
    "        # padding = 'valid', # one of \"valid\" or \"same\", same results in an output with the same size as the input\n",
    "        # dilation_rate = (1, 1), # specifying the dilation rate to use for dilated convolution\n",
    "        # use_bias = True, # Boolean, whether the layer uses a bias vector\n",
    "        # kernel_initializer = 'glorot_uniform', # Initializer for the kernel weights matrix\n",
    "        # bias_initializer = 'zeros', # Initializer for the bias vector\n",
    "        # kernel_regularizer = None, # Regularizer function applied to the kernel weights matrix\n",
    "        # bias_regularizer = None, # Regularizer function applied to the bias vector\n",
    "        # activity_regularizer = None, # Regularizer function applied to the output of the layer (its \"activation\")\n",
    "        # kernel_constraint = None, # Constraint function applied to the kernel matrix\n",
    "        # bias_constraint = None, # Constraint function applied to the bias vector\n",
    "        ))\n",
    "\n",
    "# add a second layer to the model\n",
    "model.add(\n",
    "    # flattens the Conv2D layer from a 2d-array to a 1d-array (because a dense layer needs a 1d-array)\n",
    "    Flatten()\n",
    "    )\n",
    "\n",
    "# add a third layer to the model\n",
    "model.add(\n",
    "    # regular fully-connected NN layer\n",
    "    Dense(\n",
    "        units=10, # dimensionality of the output space -> how many classes we want to predict\n",
    "        activation=sigmoid) # applies the sigmoid activation function\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples, validate on 10000 samples\n60000/60000 [==============================] - 14s 228us/sample - loss: 0.4948 - accuracy: 0.8550 - mse: 27.9267 - val_loss: 0.2539 - val_accuracy: 0.9249 - val_mse: 28.1068\n10000/10000 [==============================] - 1s 64us/sample - loss: 0.2539 - accuracy: 0.9249 - mse: 28.1068\n\n\nTest loss: 0.25\nTest accuracy: 0.92 %\nTest mse: 28.10675048828125\n"
    }
   ],
   "source": [
    "# was macht die loss function genau? Gibt es da noch andere?\n",
    "model.compile(\n",
    "    optimizer = Adam(),  # adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\n",
    "    # computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes\n",
    "    loss = SparseCategoricalCrossentropy(),\n",
    "    # metrics to be evaluated by the model during training and testing\n",
    "    metrics = [\"accuracy\"]]\n",
    ")\n",
    "\n",
    "# batch size?\n",
    "model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = 32, # Number of samples per gradient update. If unspecified, batch_size will default to 32\n",
    "    epochs = 1, # Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "    verbose = 1, # Show progress\n",
    "    validation_data = (x_test, y_test) # Data on which to evaluate the loss and any model metrics at the end of each epoch\n",
    ")\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Test loss: {:.2f}\".format(score[0]))\n",
    "print(\"Test accuracy: {:.2f} %\".format(score[1]))\n",
    "print(\"Test mse: {}\".format(score[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples, validate on 10000 samples\n60000/60000 [==============================] - 170s 3ms/sample - loss: 0.1331 - accuracy: 0.9592 - mse: 27.3878 - val_loss: 0.0422 - val_accuracy: 0.9868 - val_mse: 27.3377\n10000/10000 [==============================] - 5s 532us/sample - loss: 0.0422 - accuracy: 0.9868 - mse: 27.3377\n\n\nTest loss: 0.04\nTest accuracy: 0.99 %\nTest mse: 27.337718963623047\n"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation=relu, input_shape=input_shape),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation=relu),\n",
    "    MaxPool2D(), # Max pooling operation for spatial data\n",
    "    Dropout(0.25), # Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting\n",
    "    Flatten(),\n",
    "    Dense(512, activation=relu),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation=softmax)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(), loss = SparseCategoricalCrossentropy(), metrics = [[\"accuracy\", \"mse\"]]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = 32,\n",
    "    epochs = 1,\n",
    "    verbose = 1,\n",
    "    validation_data = (x_test, y_test)\n",
    ")\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Test loss: {:.2f}\".format(score[0]))\n",
    "print(\"Test accuracy: {:.2f} %\".format(score[1]))\n",
    "print(\"Test mse: {}\".format(score[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}