{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "CNN for MNIST - keras example.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "689jcEbMsW3j",
        "colab_type": "text"
      },
      "source": [
        "# CNN for MNIST\n",
        "\n",
        "Das Basis CNN - Projekt auf https://keras.io/examples/mnist_cnn/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdHIoiW3sW3q",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8wR-t7qsW3t",
        "colab_type": "code",
        "colab": {},
        "outputId": "950bf4ea-8b6a-43f8-ee6e-36d744f3e6f9"
      },
      "source": [
        "#from __future__ import print_function # wird bei python 3 nicht benötigt\n",
        "import keras\n",
        "from keras.datasets import mnist # unser Datensatz\n",
        "from keras.models import Sequential # damit wir überhaupt ein Model bauen können\n",
        "from keras.layers import Dense, Dropout, Flatten # für das \"standard\" Network\n",
        "from keras.layers import Conv2D, MaxPooling2D # für das Convolutional Network\n",
        "from keras import backend as K # für low-level operations (vgl. https://keras.io/backend/)\n",
        "from keras.optimizers import Adadelta # wie optimiert werden soll"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teWSnnQMsW4K",
        "colab_type": "text"
      },
      "source": [
        "## Parameter definition\n",
        "Die einzelnen Parameter als globals definiert\n",
        "* *batch* die Korrekturen die durchgeührt werden (je niedriger, desto mehr)\n",
        "* *num_classes* Wieviele Kategorien am Ende über bleiben sollen\n",
        "* *epochs* ist die Anzahl der Durchläufe\n",
        "* *input image dimensions* wie groß die einzelnen Bilder sind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yemAIVMnsW4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10 \n",
        "epochs = 1 # hier vorerst auf 1 gesetzt, damit man besser andere Parameter verändern kann\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpF2Cm3asW4Y",
        "colab_type": "text"
      },
      "source": [
        "## Train/Test Split\n",
        "\n",
        "* Zuerst der Split und gleichzeitig das Laden der Daten\n",
        "\n",
        "* Danach wird das Datenformat überprüft, denn der Input muss dementsprechend angepasst werden\n",
        " * vgl. https://keras.io/layers/convolutional/\n",
        "\n",
        "    data_format: A string, one of \"channels_last\" or \"channels_first\". The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be \"channels_last\".\n",
        "\n",
        "* Train und Test wird in float umgewandelt und durch 255 dividiert und das Ergebnis zugewiesen\n",
        "* Ausgabe wie unsere Train/Test Daten ausschauen\n",
        " * NB! x_train shape: (60000, 28, 28, 1) deutet auf das Datenformat \"channels_last\"\n",
        "\n",
        "* Anschließend muss noch der Output-vector in eine binary class matrix convertiert werden\n",
        " * vgl. https://keras.rstudio.com/reference/to_categorical.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "T3Y8-NIusW4a",
        "colab_type": "code",
        "colab": {},
        "outputId": "1d8133c5-c316-4962-ed25-0ef1b85f6a53"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS4uxA-wsW4j",
        "colab_type": "text"
      },
      "source": [
        "## Work on the Model\n",
        "\n",
        "### Create Model\n",
        "\n",
        "* Sequential ist das leere Gerüst (The Sequential model is a linear stack of layers)\n",
        "* mit .add() werden Layer hinzugefügt - dabei wird angegeben von welchem Typ der Layer ist\n",
        "  * z.B. Conv2D, oder Dense\n",
        "  * Übergeben werden:\n",
        "    * Anzahl der Nodes - selbsterklärend\n",
        "    * kernel_size - kernel sind die kleinen \"Stichproben\" die das Bild Stück für Stück durchgehen, hier wird deren Größe angegeben (Könnte man eventuell besser formulieren)\n",
        "    * activation Methode - \n",
        "    * im ersten Layer muss auch der input_shape übergeben werden\n",
        "* in unserem Fall zwei Conv. Layer (32 und 64 Nodes)\n",
        "* dann folgt MaxPooling - beim Max Pooling wird jeweils ein kleines Feld (2x2 bei uns) mit den max Werten gefüllt\n",
        "  * also eine Dimensionsreduktion\n",
        "* mit Dropout werden Nodes gestrichen um ein Overfitting zu verhindern\n",
        "  * Die Rate wird mit angegeben (0.25)\n",
        "* Flatten bedeutet, dass aus der Feature Matrix, die aus dem Pooling kommt ein Feature Vector wird\n",
        "  * wird vor einem fully connected Layer (dense) gemacht\n",
        "* Es folgt der Dense Layer - übernimmt die Ergebnisse aus dem Convolutional Layers um eine Prediction zu erzeugen\n",
        "  * bei uns sind es zwei dense Layer mit einem Dropout Zwischenschritt (0.5) bevor die wirkliche Prediction gemacht wird.\n",
        "    * ester dense Layer mit 128 Nodes und einer relu\n",
        "    * der Zweite hat (natürlich) nur noch so viele Nodes wie Nummernklassen und eine softmax activation\n",
        "      * softmax brauch ich zum klassifizieren - da sigmoid ja nur zwei Klassen hätte. Softmax bringt mir den Output auf zwischen 0 und 1 *und* schaut auch, dass die Summe der Outputs eins ist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS6nFAQAsW4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgjyba_3sW4v",
        "colab_type": "text"
      },
      "source": [
        "### Trainingsvorbereitung\n",
        "\n",
        "Nachdem das Model erstellt ist wird es hier auf das Training vorbereitet - ihm wird gesagt wie es lernen soll.\n",
        "Dazu brauchen wir:\n",
        "* Eine loss-function - in unserem Fall: categorical_crossentropy\n",
        "* Einen Optimizer - Adadelta\n",
        "* Eine Metrik nach der optimiert werden soll\n",
        "\n",
        "Es gibt eine Unzahl an Möglichkeiten hier Sachen zu verändern - und ehrlich gesagt fühle ich mich bei weitem noch nicht in der Lage hier groß mitzureden. Deshalb sollten die voreingestellten belassen werden - Adadelta und categorical_crossentropy scheinen fü dieses Problem auch die gängigsten zu sein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiOa1fXzsW4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=Adadelta(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xS28-I4sW49",
        "colab_type": "text"
      },
      "source": [
        "### Das eigentliche Training\n",
        "\n",
        "Abhängig von den Parametern kann das sehr viel Zeit in Anspruch nehmen\n",
        "\n",
        "Die fit Methode ist zum größten Teil selbsterklärend\n",
        "* verbose ist die Visualisierung des Fortschrittes (um sicher zu gehen, dass der Computer noch rechnet)\n",
        "* (0 = silent (man sieht nichts), 1 = animated progress bar ([==========]), 2 = zeigt die Epochennummer an (Epoch 1/10))\n",
        "\n",
        "Dann wird mit evaluate loss und metrics value (bei und accuracy) überprüft und ausgegeben"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly9WsUOTsW4-",
        "colab_type": "code",
        "colab": {},
        "outputId": "3331361c-33fc-4783-c82e-6e717731a494"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 194s 3ms/step - loss: 0.2670 - accuracy: 0.9187 - val_loss: 0.0627 - val_accuracy: 0.9805\n",
            "Test loss: 0.06270357351452112\n",
            "Test accuracy: 0.9804999828338623\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}